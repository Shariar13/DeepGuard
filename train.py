import os
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import open_clip
from sklearn.metrics import confusion_matrix
from tqdm import tqdm
import warnings
import random

warnings.filterwarnings("ignore")

# ==================== CONFIG ====================
class Config:
    TRAIN_REAL = "DeepGuardDB/train/real"
    TRAIN_FAKE = "DeepGuardDB/train/fake" 
    VAL_REAL = "DeepGuardDB/val/real"
    VAL_FAKE = "DeepGuardDB/val/fake"
    
    MODEL_SAVE_PATH = "consistency_detector.pth"
    BATCH_SIZE = 32
    EPOCHS = 15
    LEARNING_RATE = 1e-4
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Simple parameters
    CLIP_DIM = 1024
    REAL_PENALTY = 4.0  # Higher penalty for false positives

# ==================== DATASET ====================
class SimpleDataset(Dataset):
    def __init__(self, real_dir, fake_dir, transform=None):
        self.transform = transform
        self.images = []
        self.labels = []
        
        # All image extensions
        valid_extensions = (
            '.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff', '.tif', 
            '.webp', '.svg', '.ico', '.jfif', '.pjpeg', '.pjp', '.avif',
            '.apng', '.heic', '.heif', '.raw', '.cr2', '.nef', '.arw',
            '.dng', '.orf', '.rw2', '.pef', '.sr2', '.x3f', '.raf',
            '.3fr', '.fff', '.iiq', '.k25', '.kdc', '.mef', '.mos',
            '.mrw', '.nrw', '.ptx', '.r3d', '.rwl', '.srw', '.erf',
            '.dcr', '.crw', '.bay', '.cap', '.eip', '.dcs',
            '.dcx', '.djvu', '.eps', '.exr', '.hdr', '.j2k', '.jp2',
            '.jpx', '.mng', '.pbm', '.pcx', '.pfm', '.pgm', '.ppm',
            '.psd', '.ras', '.sgi', '.tga', '.wbmp', '.xbm', '.xpm'
        )
        
        # Load real images
        for img_name in os.listdir(real_dir):
            if img_name.lower().endswith(valid_extensions):
                self.images.append(os.path.join(real_dir, img_name))
                self.labels.append(0)  # Real = 0
        
        # Load fake images  
        for img_name in os.listdir(fake_dir):
            if img_name.lower().endswith(valid_extensions):
                self.images.append(os.path.join(fake_dir, img_name))
                self.labels.append(1)  # Fake = 1
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        try:
            image = Image.open(self.images[idx]).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, self.labels[idx]
        except:
            return torch.zeros(3, 224, 224), self.labels[idx]

# ==================== NOVEL BUT SIMPLE INNOVATION ====================
class ConsistencyDetector(nn.Module):
    """
    PATENT INNOVATION: Multi-Scale Consistency Verification
    
    Simple but Novel Idea:
    - Extract CLIP features at 3 different scales (full, half, quarter)
    - Real images have consistent features across scales
    - AI images lose consistency when downscaled due to generation artifacts
    - Use cosine similarity between scales as authenticity measure
    
    This is patent-worthy because:
    1. First to use multi-scale CLIP consistency for deepfake detection
    2. Simple but effective - leverages scale invariance property of real images
    3. Works regardless of generation method
    """
    
    def __init__(self):
        super().__init__()
        
        # Simple consistency analyzer
        self.consistency_head = nn.Sequential(
            nn.Linear(Config.CLIP_DIM, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 2)  # Real vs Fake
        )
        
        # Scale consistency scorer
        self.scale_scorer = nn.Sequential(
            nn.Linear(3, 16),  # 3 cosine similarities
            nn.ReLU(),
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, 1),
            nn.Sigmoid()
        )
        
        # Final fusion
        self.final_classifier = nn.Sequential(
            nn.Linear(Config.CLIP_DIM + 1, 128),  # +1 for scale score
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 2)
        )
    
    def forward(self, full_features, half_features, quarter_features):
        # Calculate multi-scale consistency (PATENT INNOVATION)
        cos_full_half = F.cosine_similarity(full_features, half_features, dim=1).unsqueeze(1)
        cos_full_quarter = F.cosine_similarity(full_features, quarter_features, dim=1).unsqueeze(1)
        cos_half_quarter = F.cosine_similarity(half_features, quarter_features, dim=1).unsqueeze(1)
        
        # Scale consistency score
        scale_similarities = torch.cat([cos_full_half, cos_full_quarter, cos_half_quarter], dim=1)
        scale_score = self.scale_scorer(scale_similarities)
        
        # Traditional classification
        traditional_logits = self.consistency_head(full_features)
        
        # Final prediction combining both
        fusion_input = torch.cat([full_features, scale_score], dim=1)
        final_logits = self.final_classifier(fusion_input)
        
        return final_logits, scale_score, traditional_logits

# ==================== FEATURE EXTRACTION ====================
def extract_multiscale_features(image_batch, clip_model, clip_preprocess):
    """Extract CLIP features at multiple scales"""
    batch_size = image_batch.shape[0]
    all_full_features = []
    all_half_features = []
    all_quarter_features = []
    
    for i in range(batch_size):
        # Convert tensor to PIL Image
        img_tensor = image_batch[i]
        img_pil = transforms.ToPILImage()(img_tensor)
        
        # Full scale (224x224)
        full_tensor = clip_preprocess(img_pil.resize((224, 224))).unsqueeze(0).to(Config.DEVICE)
        
        # Half scale (112x112 -> 224x224)
        half_tensor = clip_preprocess(img_pil.resize((112, 112)).resize((224, 224))).unsqueeze(0).to(Config.DEVICE)
        
        # Quarter scale (56x56 -> 224x224)
        quarter_tensor = clip_preprocess(img_pil.resize((56, 56)).resize((224, 224))).unsqueeze(0).to(Config.DEVICE)
        
        with torch.no_grad():
            if hasattr(clip_model, 'module'):
                full_feat = clip_model.module.encode_image(full_tensor)
                half_feat = clip_model.module.encode_image(half_tensor)
                quarter_feat = clip_model.module.encode_image(quarter_tensor)
            else:
                full_feat = clip_model.encode_image(full_tensor)
                half_feat = clip_model.encode_image(half_tensor)
                quarter_feat = clip_model.encode_image(quarter_tensor)
            
            # Normalize features
            full_feat = F.normalize(full_feat, dim=-1).squeeze()
            half_feat = F.normalize(half_feat, dim=-1).squeeze()
            quarter_feat = F.normalize(quarter_feat, dim=-1).squeeze()
        
        all_full_features.append(full_feat)
        all_half_features.append(half_feat)
        all_quarter_features.append(quarter_feat)
    
    return torch.stack(all_full_features), torch.stack(all_half_features), torch.stack(all_quarter_features)

# ==================== LOSS FUNCTION ====================
def consistency_loss(final_logits, scale_scores, traditional_logits, labels):
    """Simple loss with false positive penalty"""
    
    # Main loss with penalty for misclassifying real as fake
    weights = torch.where(labels == 0, Config.REAL_PENALTY, 1.0).to(Config.DEVICE)
    main_loss = F.cross_entropy(final_logits, labels, reduction='none')
    weighted_loss = (main_loss * weights).mean()
    
    # Scale consistency regularization (real images should have high consistency)
    consistency_targets = (1 - labels.float()).unsqueeze(1)  # Real=1, Fake=0
    scale_loss = F.mse_loss(scale_scores, consistency_targets)
    
    # Traditional loss
    trad_loss = F.cross_entropy(traditional_logits, labels)
    
    return weighted_loss + 0.1 * scale_loss + 0.05 * trad_loss

# ==================== TRAINING ====================
def train_epoch(model, dataloader, optimizer, epoch, clip_model, clip_preprocess):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}')
    for images, labels in pbar:
        images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)
        
        # Extract multi-scale features
        full_feat, half_feat, quarter_feat = extract_multiscale_features(images, clip_model, clip_preprocess)
        
        # Forward pass
        final_logits, scale_scores, traditional_logits = model(full_feat, half_feat, quarter_feat)
        
        # Compute loss
        loss = consistency_loss(final_logits, scale_scores, traditional_logits, labels)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Statistics
        total_loss += loss.item()
        predicted = torch.argmax(final_logits, dim=1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
        pbar.set_postfix({
            'Loss': f'{loss.item():.3f}',
            'Acc': f'{100.*correct/total:.1f}%'
        })
    
    return total_loss / len(dataloader), 100. * correct / total

def validate(model, dataloader, clip_model, clip_preprocess):
    model.eval()
    correct = 0
    total = 0
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc='Validating'):
            images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)
            
            # Extract features
            full_feat, half_feat, quarter_feat = extract_multiscale_features(images, clip_model, clip_preprocess)
            
            # Forward pass
            final_logits, scale_scores, traditional_logits = model(full_feat, half_feat, quarter_feat)
            
            predicted = torch.argmax(final_logits, dim=1)
            
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    accuracy = 100. * correct / total
    return accuracy, all_predictions, all_labels

# ==================== MAIN ====================
def main():
    print("ðŸš€ Multi-Scale Consistency Detector")
    print(f"ðŸ’» Device: {Config.DEVICE}")
    
    # Initialize CLIP
    print("ðŸ”„ Loading CLIP...")
    clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(
        'ViT-H-14', 
        pretrained='laion2b_s32b_b79k'
    )
    clip_model = clip_model.to(Config.DEVICE).eval()
    
    # Freeze CLIP
    for param in clip_model.parameters():
        param.requires_grad = False
    
    # Simple transforms
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # Load data
    print("ðŸ“ Loading data...")
    train_dataset = SimpleDataset(Config.TRAIN_REAL, Config.TRAIN_FAKE, transform)
    val_dataset = SimpleDataset(Config.VAL_REAL, Config.VAL_FAKE, transform)
    
    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=4)
    
    print(f"ðŸ“Š Train: {len(train_dataset)}, Val: {len(val_dataset)}")
    
    # Model
    model = ConsistencyDetector().to(Config.DEVICE)
    
    # Multi-GPU
    if torch.cuda.device_count() > 1:
        print(f"ðŸš€ Using {torch.cuda.device_count()} GPUs")
        model = nn.DataParallel(model)
        clip_model = nn.DataParallel(clip_model)
    
    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)
    
    print("ðŸŽ¯ Training...")
    best_acc = 0
    
    for epoch in range(Config.EPOCHS):
        train_loss, train_acc = train_epoch(model, train_loader, optimizer, epoch, clip_model, clip_preprocess)
        val_acc, val_preds, val_labels = validate(model, val_loader, clip_model, clip_preprocess)
        
        print(f"\nEpoch {epoch+1}: Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.1f}%, Val Acc: {val_acc:.1f}%")
        
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save({
                'model_state_dict': model.state_dict(),
                'accuracy': val_acc
            }, Config.MODEL_SAVE_PATH)
            print(f"âœ… Best model saved: {val_acc:.1f}%")
    
    # Final results
    print("\nðŸŽ‰ Final Results:")
    cm = confusion_matrix(val_labels, val_preds)
    tn, fp, fn, tp = cm.ravel()
    
    print(f"Confusion Matrix:")
    print(f"Real: {tn:4d} correct, {fp:4d} wrong")
    print(f"Fake: {tp:4d} correct, {fn:4d} wrong")
    print(f"False Positive Rate: {fp/(tn+fp)*100:.1f}%")
    print(f"Accuracy: {val_acc:.1f}%")

if __name__ == '__main__':
    main()
